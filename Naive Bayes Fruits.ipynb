{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, Conditional Probability & Bayes' Rule\n",
    "\n",
    "Before someone can understand and appreciate the nuances of Naive Bayes', they need to know a couple of related concepts first, namely, the idea of Conditional Probability, and Bayes' Rule. **Conditional Probability** in plain English: What is the probability that something will happen, given that something else has already happened.\n",
    "\n",
    "Let's say that there is some **Outcome O**. And some **Evidence E**. From the way these probabilities are defined: \n",
    "> The Probability of having both the Outcome O and Evidence E is: (Probability of O occurring) multiplied by the (Prob of E given that O happened)\n",
    "\n",
    "One Example to understand Conditional Probability:\n",
    "\n",
    "Let say we have a collection of US Senators. Senators could be Democrats or Republicans. They are also either male or female.\n",
    "\n",
    "If we select one senator completely randomly, what is the probability that this person is a female Democrat? Conditional Probability can help us answer that.\n",
    "\n",
    "> Probability of (Democrat and Female Senator) = Prob(Senator is Democrat) multiplied by Conditional Probability of Being Female given that they are a Democrat.\n",
    "\n",
    "<pre>\n",
    "P(Democrat & Female) = P(Democrat) * P(Female | Democrat) \n",
    "</pre>\n",
    "\n",
    "We could compute the exact same thing, the reverse way:\n",
    "\n",
    "<pre>\n",
    "P(Democrat & Female) = P(Female) * P(Democrat | Female)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bayes Rule\n",
    "\n",
    "Conceptually, this is a way to go from \n",
    "   > P(Evidence| Known Outcome) to P(Outcome|Known Evidence).\n",
    "\n",
    "Often, we know how frequently some particular evidence is observed, given a known outcome. We have to use this known fact to compute the reverse, to compute the chance of that outcome happening, given the evidence.\n",
    "\n",
    "> P(Outcome given that we know some Evidence) = P(Evidence given that we know the Outcome) times Prob(Outcome), scaled by the P(Evidence)\n",
    "\n",
    "The classic example to understand Bayes' Rule:\n",
    "<pre>\n",
    "Probability of Disease D given Test-positive =        Prob(Test is positive|Disease) * P(Disease)\n",
    "                                               _______________________________________________________________\n",
    "                                               (scaled by) Prob(Testing Positive, with or without the disease)        \n",
    "                                                        \n",
    "</pre>             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting to Naive Bayes'\n",
    "\n",
    "So far, we have talked only about one piece of evidence. In reality, we have to predict an outcome given **multiple evidence**. In that case, the math gets very complicated. To get around that complication, one approach is to 'uncouple' multiple pieces of evidence, and to treat each of piece of evidence as independent. This approach is why this is called naive Bayes.\n",
    "\n",
    "<pre>\n",
    "\n",
    "                      P(Likelihood of Evidence) * Prior prob of outcome\n",
    "P(outcome|evidence) = _________________________________________________\n",
    "                                         P(Evidence)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fruit Example\n",
    "\n",
    "Let's try it out on an example to increase our understanding: The OP asked for a 'fruit' identification example.\n",
    "\n",
    "Let's say that we have data on 1000 pieces of fruit. They happen to be **Banana, Orange** or some **Other Fruit**. We know 3 characteristics about each fruit:\n",
    "\n",
    "1. Whether it is Long\n",
    "2. Whether it is Sweet and\n",
    "3. If its color is Yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut adalah data yang diberikan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type | * | Long | Not Long | * | Sweet | Not Sweet | * | Yellow | Not Yellow | * | TOTAL | \n",
    "| :--: | :-: | :--: | :--: | :-: | :--: | :--: | :-: | :--: | :--: | :-: | :---: |\n",
    "| Banana | * | 400 | 100 | * | 350 | 150 | * | 450 | 50 | * | 500 |\n",
    "| Orange | * | 0 | 300 | * | 150 | 150 | * | 300 | 0 | * | 300 |\n",
    "| Other Fruit | * | 100 | 100 | * | 150 | 50 | * | 50 | 150 | * | 200 |\n",
    "| **TOTAL** | * | **500** | **500** | * | **650** | **350** | * | **800** | **200** | * | **1000** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian kita menghitung **Prior** probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# P(Banana)\n",
    "P_Banana = None\n",
    "\n",
    "# P(Orange)\n",
    "P_Orange = None\n",
    "\n",
    "# P(Other Fruit)\n",
    "P_Other_Fruit = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Probability of **Evidence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P(Long)\n",
    "P_Long = None\n",
    "\n",
    "# P(Sweet)\n",
    "P_Sweet = None\n",
    "\n",
    "# P(Yellow)\n",
    "P_Yellow = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of **Likelihood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P(Long|Banana)\n",
    "P_Long_Banana = None\n",
    "# P(Long|Orange)\n",
    "P_Long_Orange = None\n",
    "# P(Long|Other Fruit)\n",
    "P_Long_Other = None\n",
    "\n",
    "# P(Sweet|Banana)\n",
    "P_Sweet_Banana = None\n",
    "# P(Sweet|Orange)\n",
    "P_Sweet_Orange = None\n",
    "# P(Sweet|Other Fruit)\n",
    "P_Sweet_Other = None\n",
    "\n",
    "# P(Yellow|Banana)\n",
    "P_Yellow_Banana = None\n",
    "# P(Yellow|Orange)\n",
    "P_Yellow_Orange = None\n",
    "# P(Yellow|Other Fruit)\n",
    "P_Yellow_Other = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a Fruit, how to classify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?\n",
    "\n",
    "We can simply run the numbers for each of the 3 outcomes, one by one. Then we choose the highest probability and 'classify' our unknown fruit as belonging to the class that had the highest probability based on our prior evidence (our 1000 fruit training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P(Banana| Long, Sweet, Yellow)\n",
    "\n",
    "# P(Orange| Long, Sweet, Yellow)\n",
    "\n",
    "# P(Other fruit| Long, Sweet, Yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
